# Practice 3 & 4
This repository contains implementations and experiments for three core RL algorithms in both discrete and continuous GridWorld environments.

Implemented algorithms include:

- **REINFORCE** (Monte-Carlo policy gradient for continuous actions)
 
---

## ğŸš€ Training
To train an RL agent, run the `train.py` script with the desired algorithm and optional arguments.
```bash
python train_r.py [--map MAP_NAME] [--render]
```
**Arguments**
- --map (str, optional): Name of a custom map JSON file (e.g. custom_map.json).
- --render (flag, optional): Render the environment during training.

The trained policy will be saved in the `checkpoints/{algo}.pth` directory as a .pkl file.

**To run the TensorBoard:**
```bash
tensorboard --logdir runs/
```

TensorBoard logs include:
- **Reward Curve**
- **Epsilon Decay** (for DeepSARSA & DQN)  
- **Loss**
- **StateValueHeatmap**
- **PolicyArrows


## ğŸ” Testing / Rendering
valuate or visualize a trained policy:
```bash
python test_r.py \
  --map MAP_NAME.yaml \
  [--render]
```

- Loads `checkpoints/{algo}.pth
- Runs for a fixed number of episodes or until goal
- `--render` uses pygame to display each step

## ğŸŒ GridWorld Environments

There are two variants:

### Discrete-Action GridWorld (`gridworld_c1.py`)
- **State:** continuous \((row, col)\) in meters  
- **Actions:** 8 directions (N, NE, E, SE, S, SW, W, NW), fixed step length  
- **Rewards:**  
  - Move: **â€“1**  
  - Trap: **â€“100**, episode ends  
  - Goal: **+100**, episode ends  

### Continuous-Action GridWorld (`gridworld_c2.py`)
- **State:** continuous \((row, col)\) in meters  
- **Action:** 2D vector \(\in[-1,1]^2\) (clamped internally)  
- **Rewards:** same as discrete  

Each cell in the map can be one of:
- ğŸŸ© **Normal** (0): free to move, reward â€“1  
- ğŸ§± **Wall** (1): blocks movement, reward â€“1  
- â˜ ï¸ **Trap** (2): ends episode, reward â€“100  
- ğŸ¯ **Goal** (3): ends episode, reward +100  

---

## ğŸ“ Folder Structure

```bash
Practice4/
â”œâ”€â”€ algos/                # Algorithm implementations
â”‚   â”œâ”€â”€ td_lambda.py
â”‚   â””â”€â”€ reinforce.py
â”œâ”€â”€ configs/              # Map configuration files (YAML)
â”‚   â”œâ”€â”€ map0.yaml
â”‚   â”œâ”€â”€ map1.yaml
â”‚   â”œâ”€â”€ hw_map1.yaml
â”‚   â”œâ”€â”€ hw_map2.yaml
â”‚   â””â”€â”€ hw_map3.yaml
â”œâ”€â”€ env/                  # GridWorld environments
â”‚   â””â”€â”€ gridworld_c2.py
â”œâ”€â”€ checkpoints/          # Saved models (*.pth)
â”œâ”€â”€ outputs/              # Generated plots (heatmaps, policy arrows)
â”œâ”€â”€ runs/                 # TensorBoard logs
â”œâ”€â”€ train_r.py            # Unified training script
â”œâ”€â”€ test_r.py             # Policy evaluation / rendering script
â””â”€â”€ README.md             # This file

```
